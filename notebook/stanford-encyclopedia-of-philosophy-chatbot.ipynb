{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Stanford Encylopedia of Philosophy(SEP) Chatbot","metadata":{}},{"cell_type":"markdown","source":"**This project uses the entire scraped content from SEP (Stanford Encyclopedia of Philosophy** <br>\nLocated at this link: https://plato.stanford.edu/ <br>\n\n**USAGE:** \n* Click on the `Run All` button and ask the question in the last cell in the variable named `query`\n\n**IMPORTANT:** Please do not uncomment and run the commented out cells, if you do not wish to run the entire pipeline for some period of time. <br>\nMany of the cells are commented because they take a lot of compute and I saw they took long time to load when i tested the shared link incognito.\n\nIf you happened to like the project Iâ€™m pleased to inform you that this isn't the final iteration but merely a draft that will soon be deployable and web hostable and possibly scalable with Kubernetes.\n\nI've put a lot of thought into this project, I hope you enjoy it. :)","metadata":{}},{"cell_type":"markdown","source":"## Scraping all entries from the main page and blacklisting 404 not found ones","metadata":{"execution":{"iopub.status.busy":"2025-04-19T12:26:15.337730Z","iopub.execute_input":"2025-04-19T12:26:15.338001Z","iopub.status.idle":"2025-04-19T12:26:15.342789Z","shell.execute_reply.started":"2025-04-19T12:26:15.337983Z","shell.execute_reply":"2025-04-19T12:26:15.341353Z"}}},{"cell_type":"markdown","source":"**The entire scraping proces can last up to 15 minutes** <br>\n**Run only if you wish to know what the code is doing**","metadata":{}},{"cell_type":"code","source":"'''\nimport requests\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\n\ndef scrape_stanford_entries(start_url=\"https://plato.stanford.edu/contents.html\"):\n    entries = []\n    visited = set()\n    blacklisted_entries = []\n\n    try:\n        response = requests.get(start_url)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        for link_tag in soup.find_all('a', href=True):\n            href = link_tag['href']\n            absolute_url = urljoin(start_url, href)\n\n            if absolute_url.startswith(\"https://plato.stanford.edu/entries/\"):\n                if absolute_url not in visited:\n                    try:\n                        head_response = requests.head(absolute_url, allow_redirects=True, timeout=5)\n                        if head_response.status_code == 404:\n                            print(f\"Blacklisted 404: {absolute_url}\")\n                            blacklisted_entries.append(absolute_url)\n                            visited.add(absolute_url)\n                        else:\n                            print(f\"Found entry: {absolute_url}\") # Added print statement\n                            entries.append(absolute_url)\n                            visited.add(absolute_url)\n                    except requests.exceptions.RequestException as e:\n                        print(f\"Error checking link {absolute_url}: {e}\")\n\n    except requests.exceptions.RequestException as e:\n        print(f\"Error fetching {start_url}: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n    return sorted(list(set(entries))), sorted(list(set(blacklisted_entries)))\n\nentries, blacklisted = scrape_stanford_entries()\nprint(\"\\nFound the following entries:\")\nfor entry in entries:\n    print(entry)\nprint(f\"\\nTotal number of unique, non-404 entries found: {len(entries)}\")\n\nprint(\"\\nBlacklisted (404) entries:\")\nfor blacklisted_entry in blacklisted:\n    print(blacklisted_entry)\nprint(f\"\\nTotal number of blacklisted (404) entries: {len(blacklisted)}\")\n'''","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:43:36.230512Z","iopub.execute_input":"2025-04-19T20:43:36.230852Z","iopub.status.idle":"2025-04-19T20:43:36.239934Z","shell.execute_reply.started":"2025-04-19T20:43:36.230828Z","shell.execute_reply":"2025-04-19T20:43:36.238911Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\"\"\"\nfor _ in entries:\n    print(_)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:43:36.241467Z","iopub.execute_input":"2025-04-19T20:43:36.241809Z","iopub.status.idle":"2025-04-19T20:43:36.261842Z","shell.execute_reply.started":"2025-04-19T20:43:36.241778Z","shell.execute_reply":"2025-04-19T20:43:36.260669Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extracting text from html pages based on the url with tag cleaning, and saving it as a dictionary with key being the url and value being the processed text","metadata":{}},{"cell_type":"code","source":"\"\"\"\ndef extract_text_from_html(html_content):\n    '''\n    Uses BeautifulSoup to parse HTML and extract all visible text content.\n    '''\n    soup = BeautifulSoup(html_content, 'html.parser')\n    all_text_elements = soup.find_all(string=True)\n    visible_texts = [text.strip() for text in all_text_elements if text.strip()]\n    full_text = ' '.join(visible_texts)\n    return full_text\n\ndef scrape_text_from_urls(url_list):\n    '''\n    Scrapes the text content from a list of URLs and stores it in a dictionary.\n\n    Args:\n        url_list (list): A list of URLs to scrape.\n\n    Returns:\n        dict: A dictionary where keys are URLs and values are the extracted text.\n    '''\n    url_text_dict = {}\n    for url in url_list:\n        print(f\"Scraping text from: {url}\")\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            html_content = response.content\n            extracted_text = extract_text_from_html(html_content)\n            url_text_dict[url] = extracted_text\n            print(f\"Successfully extracted text from: {url} (length: {len(extracted_text)} characters)\")\n        except requests.exceptions.RequestException as e:\n            print(f\"Error fetching {url}: {e}\")\n        except Exception as e:\n            print(f\"An error occurred while processing {url}: {e}\")\n    return url_text_dict\n\n# Assuming you have the 'entries' list from the previous scraping steps\n# For demonstration, let's use a subset of potential entries:\nentries = entries\n\n# Scrape the text content from the entries\nurl_to_text_mapping = scrape_text_from_urls(entries)\n\n# Print the resulting dictionary (optional)\nprint(\"\\nExtracted text dictionary:\")\nfor url, text in url_to_text_mapping.items():\n    print(f\"URL: {url}\")\n    # You might want to print a snippet of the text instead of the whole thing\n    # print(f\"Text (snippet): {text[:200]}...\")\n    print(\"-\" * 20)\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:43:36.263039Z","iopub.execute_input":"2025-04-19T20:43:36.263422Z","iopub.status.idle":"2025-04-19T20:43:36.282221Z","shell.execute_reply.started":"2025-04-19T20:43:36.263389Z","shell.execute_reply":"2025-04-19T20:43:36.281240Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Oh fun! It looks like not everything was cleaned. To make or lives easier we will clean up until as well as including \"BEGIN ARTICLE HTML DO NOT MODIFY THIS LINE AND ABOVE\" in the next cell.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nif url_to_text_mapping:\n    first_key = next(iter(url_to_text_mapping.keys()))\n    first_value = url_to_text_mapping[first_key]\n    print(\"First inserted key:\", first_key)\n    print(\"First inserted value (snippet):\", first_value + \"...\")\nelse:\n    print(\"The dictionary is empty.\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:43:36.284195Z","iopub.execute_input":"2025-04-19T20:43:36.284537Z","iopub.status.idle":"2025-04-19T20:43:36.308642Z","shell.execute_reply.started":"2025-04-19T20:43:36.284516Z","shell.execute_reply":"2025-04-19T20:43:36.307696Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Here we clean everything before \"BEGIN ARTICLE HTML DO NOT MODIFY THIS LINE AND ABOVE\" and including that phrase.","metadata":{}},{"cell_type":"code","source":"'''\ndef filter_text_dictionary(url_text_dict):\n    \"\"\"\n    Filters the values in a dictionary (where values are HTML strings)\n    to remove everything up to and including the marker\n    'BEGIN ARTICLE HTML DO NOT MODIFY THIS LINE AND ABOVE'.\n\n    Args:\n        url_text_dict (dict): A dictionary with URLs as keys and HTML strings as values.\n\n    Returns:\n        dict: A new dictionary with the same keys and filtered HTML strings as values.\n              Values will be empty strings if the marker is not found.\n    \"\"\"\n    filtered_dict = {}\n    marker = \"BEGIN ARTICLE HTML DO NOT MODIFY THIS LINE AND ABOVE\"\n    for url, html_content in url_text_dict.items():\n        try:\n            index = html_content.find(marker)\n            if index != -1:\n                filtered_dict[url] = html_content[index + len(marker):].strip()\n            else:\n                print(f\"Marker not found in HTML for URL: {url}\")\n                filtered_dict[url] = \"\"\n        except Exception as e:\n            print(f\"Error processing HTML for URL {url}: {e}\")\n            filtered_dict[url] = \"\"\n    return filtered_dict\n\n# Assuming you have your url_to_text_mapping dictionary populated\nfiltered_url_to_text_mapping = filter_text_dictionary(url_to_text_mapping)\n\n# Print the first filtered value (for demonstration)\nif filtered_url_to_text_mapping:\n    first_key_filtered = next(iter(filtered_url_to_text_mapping.keys()))\n    first_value_filtered = filtered_url_to_text_mapping[first_key_filtered]\n    print(\"\\nFirst filtered value (snippet):\")\n    print(first_value_filtered[:500] + \"...\")\nelse:\n    print(\"\\nFiltered dictionary is empty.\")\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:43:36.309423Z","iopub.execute_input":"2025-04-19T20:43:36.309676Z","iopub.status.idle":"2025-04-19T20:43:36.331788Z","shell.execute_reply.started":"2025-04-19T20:43:36.309650Z","shell.execute_reply":"2025-04-19T20:43:36.330635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n# You can now use the filtered_url_to_text_mapping dictionary for further processing\n# For example, to print the filtered text for all URLs:\nprint(\"\\nFiltered text for all URLs:\")\nfor url, text in filtered_url_to_text_mapping.items():\n    print(f\"URL: {url}\")\n    print(f\"Filtered Text (snippet): {text[:200]}...\")\n    print(\"-\" * 20)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:43:36.333061Z","iopub.execute_input":"2025-04-19T20:43:36.333401Z","iopub.status.idle":"2025-04-19T20:43:36.350022Z","shell.execute_reply.started":"2025-04-19T20:43:36.333378Z","shell.execute_reply":"2025-04-19T20:43:36.349039Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## We will pickle the dictionary as we don't want to run all those scripts taking 10 minutes to finish","metadata":{}},{"cell_type":"code","source":"\"\"\"\nimport pickle\n\npickled_dictionary = filtered_url_to_text_mapping\n\noutput_path = 'pickled_dictionary.pkl'  # Choose a filename with .pkl extension\n\nwith open(output_path, 'wb') as f:\n    pickle.dump(pickled_dictionary, f)\n\nprint(f\"Dictionary saved to: {output_path}\")\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:47:40.892826Z","iopub.execute_input":"2025-04-20T08:47:40.893158Z","iopub.status.idle":"2025-04-20T08:47:40.898707Z","shell.execute_reply.started":"2025-04-20T08:47:40.893134Z","shell.execute_reply":"2025-04-20T08:47:40.897768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## We will unpickle the binary object from the kaggle working directory","metadata":{}},{"cell_type":"code","source":"import pickle\n\nfile_path = '/kaggle/working/pickled_dictionary.pkl'\nvariable_name = 'pickled_dictionary'\n\nwith open(file_path, 'rb') as f:\n    loaded_data = pickle.load(f)\n\npickled_dictionary = loaded_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:51:22.670811Z","iopub.execute_input":"2025-04-20T07:51:22.671122Z","iopub.status.idle":"2025-04-20T07:51:23.042830Z","shell.execute_reply.started":"2025-04-20T07:51:22.671100Z","shell.execute_reply":"2025-04-20T07:51:23.041712Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Installing packages and removing conflicts","metadata":{}},{"cell_type":"code","source":"!pip uninstall -qqy jupyterlab kfp  # Remove unused conflicting packages\n!pip install -qU \"google-genai==1.7.0\" \"chromadb==0.6.3\" ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:51:28.769246Z","iopub.execute_input":"2025-04-20T07:51:28.769495Z","iopub.status.idle":"2025-04-20T07:52:04.787753Z","shell.execute_reply.started":"2025-04-20T07:51:28.769478Z","shell.execute_reply":"2025-04-20T07:52:04.786954Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing google genai for LLM inference for RAG capability","metadata":{}},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nfrom IPython.display import Markdown\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:52:15.719197Z","iopub.execute_input":"2025-04-20T07:52:15.719483Z","iopub.status.idle":"2025-04-20T07:52:16.978887Z","shell.execute_reply.started":"2025-04-20T07:52:15.719462Z","shell.execute_reply":"2025-04-20T07:52:16.978226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing GOOGLE API KEY.","metadata":{}},{"cell_type":"markdown","source":"You need to import your own key. Don't worry it is free and you don't need to connect your credit card","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nGOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:52:19.581244Z","iopub.execute_input":"2025-04-20T07:52:19.581632Z","iopub.status.idle":"2025-04-20T07:52:19.771337Z","shell.execute_reply.started":"2025-04-20T07:52:19.581611Z","shell.execute_reply":"2025-04-20T07:52:19.770386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"client = genai.Client(api_key=GOOGLE_API_KEY)\n\nfor m in client.models.list():\n    if \"embedContent\" in m.supported_actions:\n        print(m.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:52:21.800591Z","iopub.execute_input":"2025-04-20T07:52:21.800814Z","iopub.status.idle":"2025-04-20T07:52:22.130852Z","shell.execute_reply.started":"2025-04-20T07:52:21.800798Z","shell.execute_reply":"2025-04-20T07:52:22.130093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing to see if the unpickled dictionary has everything that was needed","metadata":{}},{"cell_type":"code","source":"documents = [document for document in pickled_dictionary.values()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:52:26.736261Z","iopub.execute_input":"2025-04-20T07:52:26.736487Z","iopub.status.idle":"2025-04-20T07:52:26.741028Z","shell.execute_reply.started":"2025-04-20T07:52:26.736471Z","shell.execute_reply":"2025-04-20T07:52:26.739967Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#documents[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:43:36.387961Z","iopub.status.idle":"2025-04-19T20:43:36.388286Z","shell.execute_reply.started":"2025-04-19T20:43:36.388139Z","shell.execute_reply":"2025-04-19T20:43:36.388156Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Making a nicer list with the embedded url in each item","metadata":{}},{"cell_type":"markdown","source":"In case a user want to get a url of the explained page","metadata":{}},{"cell_type":"code","source":"concatenated_list = [f\"URL is {url} and the text is: {text}\" for url, text in pickled_dictionary.items()]\n\n# \"URL is https://plato.stanford.edu/entries/abduction/ and the text is: ... (extracted text) ...\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:52:30.231976Z","iopub.execute_input":"2025-04-20T07:52:30.232299Z","iopub.status.idle":"2025-04-20T07:52:30.360916Z","shell.execute_reply.started":"2025-04-20T07:52:30.232280Z","shell.execute_reply":"2025-04-20T07:52:30.359936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#concatenated_list[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T20:43:36.391460Z","iopub.status.idle":"2025-04-19T20:43:36.391752Z","shell.execute_reply.started":"2025-04-19T20:43:36.391601Z","shell.execute_reply":"2025-04-19T20:43:36.391616Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Had some problems with database ingestion so i calculated whether any list item exceeded 4MB","metadata":{}},{"cell_type":"markdown","source":"Spoiler: It didn't\nThis was a real doozy, fix explanation is in a nex few cells","metadata":{}},{"cell_type":"code","source":"# Function to calculate the size of a document in MB\ndef get_size_in_mb(document):\n    return len(document.encode('utf-8')) / (1024 * 1024)  # Size in MB\n\n# List of documents\nconcatenated_list = concatenated_list\n\n# Create a list of tuples (document, size)\ndocument_sizes = [(doc, get_size_in_mb(doc)) for doc in concatenated_list]\n\n# Sort the documents by size in descending order\ndocument_sizes_sorted = sorted(document_sizes, key=lambda x: x[1], reverse=True)\n\n# Print the sorted documents with their sizes\nfor i, (doc, size_in_mb) in enumerate(document_sizes_sorted):\n    print(f\"Document {i+1} size: {size_in_mb:.2f} MB\\nContent: {doc[:100]}...\")  # Displaying only the first 100 chars of the content for brevity\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:52:33.023530Z","iopub.execute_input":"2025-04-20T07:52:33.023774Z","iopub.status.idle":"2025-04-20T07:52:33.188391Z","shell.execute_reply.started":"2025-04-20T07:52:33.023758Z","shell.execute_reply":"2025-04-20T07:52:33.187111Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## We first need to prepare a Embedding Function to give parameters for generating vector embeddings ","metadata":{}},{"cell_type":"code","source":"from chromadb import Documents, EmbeddingFunction, Embeddings\nfrom google.api_core import retry\n\nfrom google.genai import types\n\n\n# Define a helper to retry when per-minute quota is reached.\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\n\nclass GeminiEmbeddingFunction(EmbeddingFunction):\n    # Specify whether to generate embeddings for documents, or queries\n    document_mode = True\n\n    @retry.Retry(predicate=is_retriable)\n    def __call__(self, input: Documents) -> Embeddings:\n        if self.document_mode:\n            embedding_task = \"retrieval_document\"\n        else:\n            embedding_task = \"retrieval_query\"\n\n        response = client.models.embed_content(\n            model=\"models/text-embedding-004\",\n            contents=input,\n            config=types.EmbedContentConfig(\n                task_type=embedding_task,\n            ),\n        )\n        return [e.values for e in response.embeddings]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:53:15.849259Z","iopub.execute_input":"2025-04-20T07:53:15.849520Z","iopub.status.idle":"2025-04-20T07:53:15.855187Z","shell.execute_reply.started":"2025-04-20T07:53:15.849503Z","shell.execute_reply":"2025-04-20T07:53:15.854450Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Adding items to a Chroma db vector database","metadata":{}},{"cell_type":"markdown","source":"Ah yes, the bane of my existence. So many iterations, so many official solutions that failed and LLM massaging giving me always something more with cryptic error messages in the stack trace. <br>\nFor me in the end to settle with a hacky solution with slowly iterating and adding a timed pause between requests to a google database.\n\nIt is slower, but it works. I guess that is better than not working at all. <br>\nAnd the good news is it only needs to be ran when you want to update the database. <br>\nThe SEP team rarely does as they are super picky what they let in and who they let edit ther knowledgebase. <br>\n\nFor example: it took quite a bit of time for Rudolf Carnap to get his article becuse he is such an important figure and it required a massive undertaking collectivizing his work in a encyclopedia article.","metadata":{}},{"cell_type":"code","source":"\"\"\"\nDB_NAME = \"googlecardb\"\ndocuments = concatenated_list\n\nembed_fn = GeminiEmbeddingFunction()\nembed_fn.document_mode = True\n\nchroma_client = chromadb.Client()\ndb = chroma_client.get_or_create_collection(name=DB_NAME, embedding_function=embed_fn)\n\nfor i, doc in enumerate(documents):\n    db.add(documents=[doc], ids=[str(i)])\n    time.sleep(1) # Add a delay after each successful addition\n    print(f\"Added document with ID: {i}, Content (first 100 chars): {str(doc[:100])}\")\n\nprint(f\"\\nFinished adding {len(documents)} documents to the '{DB_NAME}' collection.\")\n\"\"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T07:55:05.702245Z","iopub.execute_input":"2025-04-20T07:55:05.702775Z","iopub.status.idle":"2025-04-20T08:33:33.177862Z","shell.execute_reply.started":"2025-04-20T07:55:05.702757Z","shell.execute_reply":"2025-04-20T08:33:33.177064Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Querying the database","metadata":{}},{"cell_type":"markdown","source":"We now hope and pray everything is in order in the DB","metadata":{}},{"cell_type":"code","source":"db.count()\n# You can peek at the data too.\ndb.peek(1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:35:27.357542Z","iopub.execute_input":"2025-04-20T08:35:27.357800Z","iopub.status.idle":"2025-04-20T08:35:27.376852Z","shell.execute_reply.started":"2025-04-20T08:35:27.357782Z","shell.execute_reply":"2025-04-20T08:35:27.376075Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Testing the embeddings","metadata":{}},{"cell_type":"code","source":"# Switch to query mode when generating embeddings.\nembed_fn.document_mode = False\n\n# Search the Chroma DB using the specified query.\nquery = \"Explain Abelards Logic?\"\n\nresult = db.query(query_texts=[query], n_results=1)\n[all_passages] = result[\"documents\"]\n\nMarkdown(all_passages[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:46:15.661359Z","iopub.execute_input":"2025-04-20T08:46:15.661623Z","iopub.status.idle":"2025-04-20T08:46:15.939286Z","shell.execute_reply.started":"2025-04-20T08:46:15.661604Z","shell.execute_reply":"2025-04-20T08:46:15.938458Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Giving background using prompting strategies with instructions how to behave and answer but using zero shot prompting","metadata":{}},{"cell_type":"code","source":"query_oneline = query.replace(\"\\n\", \" \")\n\n# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\nprompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. You answer with the knowledge of a philosophy postdoc that is trying to bridge complicated subjects to a non-philosophy knowing audience, so be sure to break down complicated concepts and strike a friendly and converstional tone.\nBe sure to respond in a complete sentence, being comprehensive, including all relevant background information. \nIf the passage is irrelevant to the answer, you may ignore it.\n\nQUESTION: {query_oneline}\n\"\"\"\n\n# Add the retrieved documents to the prompt.\nfor passage in all_passages:\n    passage_oneline = passage.replace(\"\\n\", \" \")\n    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n\nprint(prompt)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T09:56:10.843536Z","iopub.execute_input":"2025-04-20T09:56:10.843935Z","iopub.status.idle":"2025-04-20T09:56:10.851363Z","shell.execute_reply.started":"2025-04-20T09:56:10.843910Z","shell.execute_reply":"2025-04-20T09:56:10.849642Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Generating the answer","metadata":{}},{"cell_type":"code","source":"answer = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=prompt)\n\nMarkdown(answer.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T08:46:23.768661Z","iopub.execute_input":"2025-04-20T08:46:23.768908Z","iopub.status.idle":"2025-04-20T08:46:25.267765Z","shell.execute_reply.started":"2025-04-20T08:46:23.768891Z","shell.execute_reply":"2025-04-20T08:46:25.267064Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Everything in one place","metadata":{}},{"cell_type":"code","source":"# Search the Chroma DB using the specified query.\nquery = \"Explain Abelards Logic.\"  #< --- Type your question here\n\n# Switch to query mode when generating embeddings.\nembed_fn.document_mode = False\n\nresult = db.query(query_texts=[query], n_results=1)\n[all_passages] = result[\"documents\"]\n\n#Markdown(all_passages[0])\n\nquery_oneline = query.replace(\"\\n\", \" \")\n\n# This prompt is where you can specify any guidance on tone, or what topics the model should stick to, or avoid.\nprompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passage included below. You answer with the knowledge of a philosophy postdoc that is trying to bridge complicated subjects to a non-philosophy knowing audience, so be sure to break down complicated concepts and strike a friendly and converstional tone.\nBe sure to respond in a complete sentence, being comprehensive, including all relevant background information. \nIf the passage is irrelevant to the answer, you may ignore it.\nAnswer no less than 100 words.\n\nQUESTION: {query_oneline}\n\"\"\"\n\n# Add the retrieved documents to the prompt.\nfor passage in all_passages:\n    passage_oneline = passage.replace(\"\\n\", \" \")\n    prompt += f\"PASSAGE: {passage_oneline}\\n\"\n\n#print(prompt)\n\nanswer = client.models.generate_content(\n    model=\"gemini-2.0-flash\",\n    contents=prompt)\n\nMarkdown(answer.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-20T09:57:05.005515Z","iopub.execute_input":"2025-04-20T09:57:05.005796Z","iopub.status.idle":"2025-04-20T09:57:08.828947Z","shell.execute_reply.started":"2025-04-20T09:57:05.005773Z","shell.execute_reply":"2025-04-20T09:57:08.828156Z"}},"outputs":[],"execution_count":null}]}